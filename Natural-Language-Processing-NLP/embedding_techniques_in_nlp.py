# -*- coding: utf-8 -*-
"""embedding_techniques_in_nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ouMqjE1d9SJi-OMflbXPp9nRDColkfGp

## What is Embedding in NLP?

In Natural Language Processing (NLP), **embeddings** are dense vector representations of words, phrases, or even entire documents. They capture semantic relationships between words, meaning that words with similar meanings will have similar vector representations in a high-dimensional space.

Think of it like this: instead of representing each word as a unique, isolated symbol (like a one-hot encoded vector where each word has its own dimension and all other dimensions are zero), embeddings map words into a continuous vector space.

Here's why they are important:

* **Capture Semantic Meaning:** Words with similar meanings are closer together in the vector space. For example, the vectors for "king" and "queen" might be similar, and the vector for "man" minus the vector for "woman" might be roughly equal to the vector for "king" minus the vector for "queen". This allows models to understand relationships between words.
* **Dimensionality Reduction:** Embeddings are typically much lower in dimensionality than one-hot encoding, making them more computationally efficient.
* **Improved Performance:** Using embeddings as input to NLP models (like neural networks) generally leads to better performance on various tasks, such as text classification, sentiment analysis, machine translation, and question answering.

Examples of popular word embedding techniques include:

* **Word2Vec:** Learns word embeddings by predicting neighboring words in a sentence.
* **GloVe (Global Vectors for Word Representation):** Learns embeddings by considering global word-word co-occurrence statistics from a corpus.
* **FastText:** An extension of Word2Vec that considers character n-grams, allowing it to handle out-of-vocabulary words and morphological variations better.
* **Contextual Embeddings (like ELMo, BERT, GPT):** These embeddings are dynamic and change based on the context in which a word appears in a sentence, capturing more nuanced meanings.

In essence, embeddings provide a way for computers to understand and process human language in a more meaningful and efficient way.
"""

corpus = [
    "The quick brown fox jumps over the lazy dog.",
    "Never jump over the lazy dog quickly.",
    "A fast brown fox leaps across lazy hounds.",
    "The sleepy cat curled up on the warm mat.",
    "A sly fox sneaked past the watchful owl.",
    "Gentle deer graze peacefully near the quiet stream.",
    "Two clever foxes raced across the frosty field.",
    "The curious rabbit hopped into the garden silently.",
    "A playful puppy chased the fluttering butterflies.",
    "Brown bears hibernate through the cold winter nights.",
    "Crows cawed loudly above the tall pine trees.",
    "A pack of wolves howled at the pale moonlight.",
    "The silent snake slithered across the dusty path.",
    "Early birds chirped merrily as the sun rose.",
    "The strong lion roared from his rocky throne."
]

"""### TF-IDF (Term Frequency-Inverse Document Frequency)

TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. It is calculated by multiplying two metrics:

*   **Term Frequency (TF):** How often a word appears in a document.
*   **Inverse Document Frequency (IDF):** A measure of how much information the word provides, i.e., if it's common or rare across all documents.

The resulting TF-IDF score increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.
"""

#@title TF-IDF (Term Frequency-Inverse Document Frequency)

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# Fit and transform
tfidf_matrix = vectorizer.fit_transform(corpus)

# Convert to DataFrame for readability
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())
print(df_tfidf.round(2))

#@title Get feature importance for first sentence
first_doc_scores = df_tfidf.iloc[0]
important_words = first_doc_scores.sort_values(ascending=False)
print("Top TF-IDF words in first sentence:\n", important_words.head())

pip install gensim

import nltk
nltk.download('all')

"""### Word2Vec

Word2Vec is a popular technique for learning word embeddings. It creates dense vector representations of words where words with similar meanings are located closer to each other in the vector space. There are two main architectures:

*   **Skip-gram:** Predicts the surrounding context words given a target word.
*   **CBOW (Continuous Bag of Words):** Predicts the target word given the surrounding context words.

Word2Vec models learn these embeddings by training on a large corpus of text, where the relationships between words are captured through their co-occurrence patterns.
"""

#@title Word2Vec (Contextual Neural Embeddings)
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

# Tokenize corpus
tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]

# Train Word2Vec model (skip-gram by default if sg=1)
model_w2v = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=2, sg=1)

# Access vector
print("Vector for 'fox':\n", model_w2v.wv['fox'])

# Most similar words
print("Most similar to 'lazy':", model_w2v.wv.most_similar('lazy'))

# Cosine similarity between words
similarity = model_w2v.wv.similarity('fox', 'dog')
print("Similarity between 'lazy' and 'dog':", similarity)

"""To use GloVe vectors, you'll need to download pretrained embeddings like: `GloVe 6B (50D)`

download link- https://nlp.stanford.edu/data/glove.6B.zip

### GloVe (Global Vectors for Word Representation)

GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Unlike Word2Vec which focuses on local context, GloVe learns word embeddings by considering global word-word co-occurrence statistics from a large corpus.

It essentially combines the advantages of two main models:

*   **Global Matrix Factorization:** Like Latent Semantic Analysis (LSA), it captures global statistics of how often words appear together across the entire corpus.
*   **Local Context Window Methods:** Like Word2Vec, it also considers the context of words within a sliding window.

The training objective of GloVe is to learn word vectors such that their dot product is equal to the logarithm of the words' co-occurrence probability. This allows GloVe to capture both semantic and syntactic relationships between words effectively.
"""

#@title GloVe (Pretrained Global Vectors)
import numpy as np

# Load pretrained GloVe vectors
def load_glove(path):
    embeddings = {}
    with open(path, 'r', encoding='utf8') as file:
        for line in file:
            parts = line.split()
            word = parts[0]
            vector = np.array(parts[1:], dtype='float32')
            embeddings[word] = vector
    return embeddings

glove_path = "/content/glove.6B.50d.txt"  # Download and place here
glove_embeddings = load_glove(glove_path)

# Check vector for a word
print("GloVe vector for 'fox':", glove_embeddings['fox'])

#@title Explore Important GloVe Features
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Cosine similarity between two vectors
vec_fox = glove_embeddings['fox']
vec_dog = glove_embeddings['dog']
similarity = cosine_similarity([vec_fox], [vec_dog])[0][0]
print("Similarity (fox vs dog):", similarity)

# Word analogy (king - man + woman â‰ˆ queen)
def analogy(word_a, word_b, word_c):
    vec = glove_embeddings[word_b] - glove_embeddings[word_a] + glove_embeddings[word_c]
    return vec

analogy_vec = analogy('man', 'king', 'woman')

# Find closest word
def find_closest(vec, embeddings):
    sims = {}
    # Filter out embeddings with incorrect dimensions
    filtered_embeddings = {word: vector for word, vector in embeddings.items() if vector.shape[0] == vec.shape[0]}
    for word in filtered_embeddings:
        sims[word] = cosine_similarity([vec], [filtered_embeddings[word]])[0][0]
    return sorted(sims.items(), key=lambda x: x[1], reverse=True)[:5]

print("Analogy result (king - man + woman):", find_closest(analogy_vec, glove_embeddings))

"""### FastText

FastText is another word embedding model that builds upon Word2Vec. A key difference is that FastText considers subword information (character n-grams) in addition to words. This has several advantages:

*   **Handles Out-of-Vocabulary (OOV) words:** FastText can create vectors for words it hasn't seen during training by combining the vectors of its character n-grams.
*   **Better for morphologically rich languages:** Languages with many word variations (like Spanish or German) benefit from the subword information.
*   **Learns representations for suffixes and prefixes:** This can help in understanding the meaning of words.

FastText can also be used for text classification, often outperforming traditional methods due to its ability to capture subword information.
"""

#@title FastText Embedding
from gensim.models import FastText
from nltk.tokenize import word_tokenize

# Tokenize sentences
tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]

# Train FastText model
fasttext_model = FastText(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=2, sg=1)

# Get word vector
print("Vector for 'fox':\n", fasttext_model.wv['fox'])

# Handle out-of-vocabulary word
print("Vector for 'foxlike' (OOV):\n", fasttext_model.wv['foxlike'])

# Similar words
print("Words similar to 'lazy':", fasttext_model.wv.most_similar('lazy'))

# Cosine similarity
print("Similarity between 'fox' and 'dog':", fasttext_model.wv.similarity('fox', 'dog'))

pip install -q transformers sentence-transformers

"""### BERT (Bidirectional Encoder Representations from Transformers)

BERT is a state-of-the-art, pre-trained transformer-based model for NLP. Unlike traditional word embeddings like Word2Vec or GloVe that generate a single vector representation for each word regardless of its context, BERT produces contextualized embeddings. This means the vector representation for a word changes based on the surrounding words in a sentence.

Key features of BERT:

*   **Bidirectional:** BERT is trained to consider the context from both the left and right sides of a word simultaneously, which leads to a deeper understanding of the word's meaning.
*   **Transformer Architecture:** BERT utilizes the transformer architecture, which is highly effective at capturing long-range dependencies in text.
*   **Pre-trained:** BERT is pre-trained on a massive dataset of text (like books and Wikipedia), allowing it to learn a rich understanding of language. This pre-training can then be fine-tuned for specific downstream NLP tasks.

Contextual embeddings like BERT have significantly advanced the performance of various NLP tasks by providing more nuanced and context-aware representations of words and sentences.
"""

#@title BERT Embedding (Contextual Embedding using Transformers)
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load pre-trained BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Get sentence embeddings
embeddings = model.encode(corpus)

# Cosine similarity between first and second sentence
sim_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
print(f"Cosine similarity (Sentence 1 vs 2): {sim_score:.4f}")

# Cosine similarity between first and second sentence
sim_score = cosine_similarity([embeddings[3]], [embeddings[3]])[0][0]
print(f"Cosine similarity (Sentence 3 vs 4): {sim_score:.4f}")

