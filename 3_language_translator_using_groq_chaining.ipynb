{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXm66UwvKkzRn9+oUhsclO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rani-sikdar/GenAI-complete-course-codes/blob/main/3_language_translator_using_groq_chaining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQk3e5DVwFDf"
      },
      "outputs": [],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY= \"sk-proj-*****\"\n",
        "LANGCHAIN_API_KEY =\"****\"\n",
        "GROQ_API_KEY= \"*****\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"LANGCHAIN_PROJECT\"\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
      ],
      "metadata": {
        "id": "igGzMnN5wKCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_groq langchain_core"
      ],
      "metadata": {
        "id": "cvEbnNnmwf4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai"
      ],
      "metadata": {
        "id": "dSQYq8may7Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq  # if we want to use openai or any other from opensource\n",
        "from langchain_openai import ChatOpenAI  # if we want to use openai from langchain\n"
      ],
      "metadata": {
        "id": "9IDCthjLyag2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGroq(model=\"gemma2-9b-it\")\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ohuiSPLy2S8",
        "outputId": "bd107f99-39d1-45d9-eae3-a21df7ba545a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7e74cffa74d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e74cffa5290>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# HumanMessage --> messages provided by the human being/user\n",
        "# SystemMessage --> instruction to the LLM --> how the LLM model should work like\n",
        "\n",
        "messages =[\n",
        "     SystemMessage(content=\"Translate the following from english to french.\"),\n",
        "     HumanMessage(content= \"Hello, how are you?\")\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mCL6v0lzbpG",
        "outputId": "7ebde46b-34c4-4631-9ad2-1c9399e130ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Bonjour, comment allez-vous ? \\n\\n\\nYou can also say:\\n\\n* Salut, ça va ? (More informal)\\n* Comment vas-tu ? (More informal, used with someone you know well)\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 23, 'total_tokens': 69, 'completion_time': 0.083636364, 'prompt_time': 0.001321131, 'queue_time': 0.08638046399999999, 'total_time': 0.084957495}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--18f85d1c-f1be-4e7f-807b-9e1f57c1d4f3-0', usage_metadata={'input_tokens': 23, 'output_tokens': 46, 'total_tokens': 69})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title one way to access the content of output response\n",
        "\n",
        "res = model.invoke(messages)\n",
        "print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObaRcTMx0zl0",
        "outputId": "21ae0609-91b4-4044-b618-749b672fc33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour, comment allez-vous ?  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title another way to access the content of output response - Output Parser\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "parser.invoke(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z_fYfE6t1L3C",
        "outputId": "54cb3a15-4c3e-4612-fff8-94f9a5c9dda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bonjour, comment allez-vous ?  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Chaining components - LCEL\n",
        "\n",
        "chain = model | parser\n",
        "chain.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e1owUa4m1tM2",
        "outputId": "e31150d1-8a21-450a-c0a2-415222723cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bonjour, comment allez-vous ? \\n\\n\\nOr, more casually:\\n\\nSalut, ça va ?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prompt template\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "generic_template = \"Translate the following into {language}:\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", generic_template),\n",
        "        (\"human\",\"{text}\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "srY1qKph18KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = prompt.invoke({\"language\": \"French\", \"text\": \"hello\"})"
      ],
      "metadata": {
        "id": "iH3Dddo2298p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_b-cyzN2_AM",
        "outputId": "9a52172e-9e0a-435e-d4d5-cb067c4de27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following into French:', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hello', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title combining promp-model-parser into a chain\n",
        "\n",
        "chain = prompt | model | parser\n",
        "chain.invoke({\"language\": \"French\", \"text\": \"hello\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ehq__y1H3OMi",
        "outputId": "d8e2f3f4-cc16-45ba-d9b0-47f9696ce7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bonjour  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s2HscDww3jdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}