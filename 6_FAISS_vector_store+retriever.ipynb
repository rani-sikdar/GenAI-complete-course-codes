{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4noO028B3oY"
      },
      "outputs": [],
      "source": [
        "pip install langchain_community faiss-cpu openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community"
      ],
      "metadata": {
        "id": "Jr1jhqKOCb9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-text-splitters langchain-openai"
      ],
      "metadata": {
        "id": "6BVm7YKCCOQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY= \"sk-----\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "Hsvbm4ocCLJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import libraries\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "YGSWAegbB_jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load the data\n",
        "\n",
        "loader= TextLoader('/content/speech.txt')\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "docs= text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxxE_oEbCjhD",
        "outputId": "5187b83d-c2a4-41b9-ac8b-7bed39e6807f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 311, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 552, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 202, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 355, which is longer than the specified 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lh4ITLlDgmK",
        "outputId": "39d6e102-5557-4005-9e46-f0fe2f546a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/speech.txt'}, page_content='The paper \"Efficient Estimation of Word Representations in Vector Space\" by Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean (2013) is a foundational paper that introduced Word2Vec, a highly efficient and effective method for learning word embeddings (also known as word representations or word vectors).'),\n",
              " Document(metadata={'source': '/content/speech.txt'}, page_content=\"Before this paper, various methods existed for creating word representations, often based on complex neural network language models (NNLMs) or statistical methods like Latent Semantic Analysis (LSA). These methods were computationally expensive, especially for large datasets and vocabularies. Word2Vec revolutionized the field by demonstrating that high-quality word vectors could be learned much more efficiently.\\nb) Continuous Skip-gram Model\\nIdea: Predicts the surrounding context words given the current word. It's essentially the inverse of CBOW.\"),\n",
              " Document(metadata={'source': '/content/speech.txt'}, page_content='Architecture (simplified):\\n\\nInput Layer: Takes the one-hot encoded vector of the target word.\\n\\nProjection Layer: This word vector is projected into a continuous vector space.'),\n",
              " Document(metadata={'source': '/content/speech.txt'}, page_content=\"Output Layer: The model then tries to predict the words within a certain window around the target word. For each context word in the window, it calculates its probability given the target word's vector.\"),\n",
              " Document(metadata={'source': '/content/speech.txt'}, page_content='Training Objective: Maximize the sum of the log probabilities of context words given the target word.'),\n",
              " Document(metadata={'source': '/content/speech.txt'}, page_content='Performance: The paper found that Skip-gram generally works better for capturing semantic relationships, especially with large datasets, while CBOW can be faster to train.'),\n",
              " Document(metadata={'source': '/content/speech.txt'}, page_content=\"3. Key Optimization Techniques for Efficiency\\nThe primary reason for Word2Vec's efficiency lies in the clever optimization techniques applied to the output layer, where the vocabulary size (V) can be enormous (millions of words). Computing a softmax over V words is computationally very expensive. The paper introduces two main techniques to address this:\"),\n",
              " Document(metadata={'source': '/content/speech.txt'}, page_content='In summary, this paper introduced Word2Vec, a set of highly efficient algorithms (CBOW and Skip-gram) for learning high-quality word embeddings. Its effectiveness stems from simplified model architectures and clever optimization techniques like hierarchical softmax and negative sampling, which enabled training on unprecedentedly large text corpora.')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Embedding\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model =\"text-embedding-3-small\", openai_api_key= OPENAI_API_KEY)\n",
        "\n",
        "db= FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "c8ttSTf2DiiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x_EbnZhD9Ts",
        "outputId": "233b57a2-7e33-4b54-d8d3-13c05b11b8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x7d14d741ee50>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title querying the vector database\n",
        "\n",
        "query =\"what is the main idea behind Continuous Skip-gram Model\"\n",
        "\n",
        "docs = db.similarity_search(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4EYnDp6D-ga",
        "outputId": "79669efd-68ed-4d08-d8ae-51846bc454f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='7a603cfe-0571-4afc-bd55-ce7e90d48617', metadata={'source': '/content/speech.txt'}, page_content=\"Before this paper, various methods existed for creating word representations, often based on complex neural network language models (NNLMs) or statistical methods like Latent Semantic Analysis (LSA). These methods were computationally expensive, especially for large datasets and vocabularies. Word2Vec revolutionized the field by demonstrating that high-quality word vectors could be learned much more efficiently.\\nb) Continuous Skip-gram Model\\nIdea: Predicts the surrounding context words given the current word. It's essentially the inverse of CBOW.\"),\n",
              " Document(id='38cfdc88-0241-4972-9b9e-ab7b50e5a72d', metadata={'source': '/content/speech.txt'}, page_content='Performance: The paper found that Skip-gram generally works better for capturing semantic relationships, especially with large datasets, while CBOW can be faster to train.'),\n",
              " Document(id='43fe5acf-3269-42ea-9dcc-5e5c9384f322', metadata={'source': '/content/speech.txt'}, page_content='In summary, this paper introduced Word2Vec, a set of highly efficient algorithms (CBOW and Skip-gram) for learning high-quality word embeddings. Its effectiveness stems from simplified model architectures and clever optimization techniques like hierarchical softmax and negative sampling, which enabled training on unprecedentedly large text corpora.'),\n",
              " Document(id='b51b3b97-726e-4d96-a3d4-b9e8a33fdd8b', metadata={'source': '/content/speech.txt'}, page_content='Architecture (simplified):\\n\\nInput Layer: Takes the one-hot encoded vector of the target word.\\n\\nProjection Layer: This word vector is projected into a continuous vector space.')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH0fMHUlLu4u",
        "outputId": "00c1eb97-a3ad-4750-85b1-4362f43c5e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "mZYoXl33EBgz",
        "outputId": "ac16a4d0-11ce-4ed3-8964-20e82e3d4c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Before this paper, various methods existed for creating word representations, often based on complex neural network language models (NNLMs) or statistical methods like Latent Semantic Analysis (LSA). These methods were computationally expensive, especially for large datasets and vocabularies. Word2Vec revolutionized the field by demonstrating that high-quality word vectors could be learned much more efficiently.\\nb) Continuous Skip-gram Model\\nIdea: Predicts the surrounding context words given the current word. It's essentially the inverse of CBOW.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "iO1WuPwaEgRY",
        "outputId": "72b1dfc3-d063-43e0-a3a8-a58448c295e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Performance: The paper found that Skip-gram generally works better for capturing semantic relationships, especially with large datasets, while CBOW can be faster to train.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert vector store into Retriever class\n",
        "# retriever act as an interface ....  retrive the details from vector store via \"query\"\n",
        "\n",
        "retriever = db.as_retriever() # converting vector into retriever.....\n",
        "\n",
        "docs= retriever.invoke(query)\n",
        "docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "WnfaqzrbEwgb",
        "outputId": "dcfb48c1-93c6-45b8-b106-4044193fd36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Before this paper, various methods existed for creating word representations, often based on complex neural network language models (NNLMs) or statistical methods like Latent Semantic Analysis (LSA). These methods were computationally expensive, especially for large datasets and vocabularies. Word2Vec revolutionized the field by demonstrating that high-quality word vectors could be learned much more efficiently.\\nb) Continuous Skip-gram Model\\nIdea: Predicts the surrounding context words given the current word. It's essentially the inverse of CBOW.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Similarity score with FAISS (returns manhattan score- L2)\n",
        "\n",
        "docs_and_score = db.similarity_search_with_score(query)\n",
        "docs_and_score[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAZJb7UYEy-q",
        "outputId": "582d2c99-275b-4bd5-d2e6-c2838aa8a9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Document(id='38cfdc88-0241-4972-9b9e-ab7b50e5a72d', metadata={'source': '/content/speech.txt'}, page_content='Performance: The paper found that Skip-gram generally works better for capturing semantic relationships, especially with large datasets, while CBOW can be faster to train.'),\n",
              " np.float32(0.9252607))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title passing vectors instead of sentences\n",
        "\n",
        "embedding_vector = embeddings.embed_query(query)\n",
        "embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQQDEparNJgz",
        "outputId": "d95b38a6-73a6-4c5e-f647-a1f1e0536de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.040307268500328064,\n",
              " 0.01383289322257042,\n",
              " 0.006966611370444298,\n",
              " -0.002762189134955406,\n",
              " -0.04765638709068298,\n",
              " 0.029496794566512108,\n",
              " 0.060398198664188385,\n",
              " -0.010145793668925762,\n",
              " -0.06446153670549393,\n",
              " 0.028518585488200188,\n",
              " -0.0041542574763298035,\n",
              " -0.03892774134874344,\n",
              " -0.01842295564711094,\n",
              " 0.03147829696536064,\n",
              " 0.009562630206346512,\n",
              " 0.003787428606301546,\n",
              " 0.014133880846202374,\n",
              " 0.016516700387001038,\n",
              " 0.05939490720629692,\n",
              " 0.03338455408811569,\n",
              " 0.022122595459222794,\n",
              " 0.02118200995028019,\n",
              " -0.007857033051550388,\n",
              " 0.03373570367693901,\n",
              " 0.01654178276658058,\n",
              " -7.20626485417597e-05,\n",
              " 0.03127763792872429,\n",
              " 0.0696786567568779,\n",
              " 0.02209751307964325,\n",
              " -0.010296287946403027,\n",
              " -0.027214305475354195,\n",
              " -0.018347708508372307,\n",
              " -0.04341747611761093,\n",
              " -0.037748873233795166,\n",
              " 0.007010505069047213,\n",
              " 0.005913154222071171,\n",
              " 0.0022009725216776133,\n",
              " 0.03679574653506279,\n",
              " -0.014472492039203644,\n",
              " 0.022636784240603447,\n",
              " 0.021884314715862274,\n",
              " 0.01659194752573967,\n",
              " -0.0027951097581535578,\n",
              " 0.03428751602768898,\n",
              " 0.03501490131020546,\n",
              " 0.07469511777162552,\n",
              " 0.022022267803549767,\n",
              " -0.019789941608905792,\n",
              " -0.0016131059965118766,\n",
              " 0.0645618662238121,\n",
              " -0.046803586184978485,\n",
              " 0.012440824881196022,\n",
              " -0.005245337728410959,\n",
              " -0.020241422578692436,\n",
              " -0.04359305277466774,\n",
              " -0.03142813220620155,\n",
              " 0.00783822126686573,\n",
              " -0.028317926451563835,\n",
              " 0.014422327280044556,\n",
              " -0.018259920179843903,\n",
              " 0.049688052386045456,\n",
              " -0.0346386693418026,\n",
              " 0.011167897842824459,\n",
              " 0.005207714159041643,\n",
              " -0.030098769813776016,\n",
              " -0.013983387500047684,\n",
              " -0.046201612800359726,\n",
              " 0.022273089736700058,\n",
              " -0.05131840333342552,\n",
              " 0.013519364409148693,\n",
              " 0.005003920756280422,\n",
              " 0.021407751366496086,\n",
              " -0.016441453248262405,\n",
              " -0.00632387725636363,\n",
              " -0.011757331900298595,\n",
              " 0.028794489800930023,\n",
              " 0.029471712186932564,\n",
              " 0.015049384906888008,\n",
              " 0.033886197954416275,\n",
              " 0.016190629452466965,\n",
              " 0.008402572944760323,\n",
              " -0.0021288609132170677,\n",
              " 0.032531753182411194,\n",
              " -0.03714689984917641,\n",
              " -0.03865183889865875,\n",
              " -0.016692277044057846,\n",
              " 0.00979464128613472,\n",
              " 0.030073687434196472,\n",
              " -0.006960340775549412,\n",
              " -0.023790569975972176,\n",
              " -0.009224019013345242,\n",
              " 0.020479705184698105,\n",
              " 0.007618751376867294,\n",
              " 0.022699490189552307,\n",
              " 0.05302400141954422,\n",
              " -0.011989343911409378,\n",
              " -0.025257885456085205,\n",
              " -0.0773036777973175,\n",
              " 0.040683504194021225,\n",
              " 0.018222296610474586,\n",
              " -0.027289552614092827,\n",
              " -0.022197842597961426,\n",
              " 0.015275126323103905,\n",
              " 0.0007791192037984729,\n",
              " 0.002970685949549079,\n",
              " -0.01960182376205921,\n",
              " 0.011581756174564362,\n",
              " 0.018598532304167747,\n",
              " -0.05187021568417549,\n",
              " -0.01960182376205921,\n",
              " -0.034061774611473083,\n",
              " -0.014485033228993416,\n",
              " -0.010929616168141365,\n",
              " 0.026411671191453934,\n",
              " 0.032180603593587875,\n",
              " -0.028543667867779732,\n",
              " -0.025809695944190025,\n",
              " 0.026311341673135757,\n",
              " 0.0055776783265173435,\n",
              " 0.014936515130102634,\n",
              " -0.027139058336615562,\n",
              " 0.010358993895351887,\n",
              " -0.06049852818250656,\n",
              " -0.026361506432294846,\n",
              " -0.013544446788728237,\n",
              " -0.014008468948304653,\n",
              " 0.06115067005157471,\n",
              " -0.01605267822742462,\n",
              " -0.004464650992304087,\n",
              " 0.012534883804619312,\n",
              " 0.011280768550932407,\n",
              " -0.04251451417803764,\n",
              " -0.0015880236169323325,\n",
              " 0.003796834498643875,\n",
              " -0.03197994455695152,\n",
              " 0.026236094534397125,\n",
              " -0.04951247572898865,\n",
              " 0.019012389704585075,\n",
              " -0.05099233239889145,\n",
              " 0.041059739887714386,\n",
              " 0.008609502576291561,\n",
              " -0.02376548759639263,\n",
              " -0.035792455077171326,\n",
              " 0.03330930694937706,\n",
              " 0.011124003678560257,\n",
              " 0.011004863306879997,\n",
              " -0.007493339944630861,\n",
              " 0.04592570662498474,\n",
              " -0.00181376445107162,\n",
              " 0.03383603319525719,\n",
              " 0.013343787752091885,\n",
              " 0.007217434234917164,\n",
              " -0.046327024698257446,\n",
              " 0.008546796627342701,\n",
              " -0.026963481679558754,\n",
              " -0.010960969142615795,\n",
              " -0.03476408123970032,\n",
              " -0.010860639624297619,\n",
              " 0.04452109709382057,\n",
              " 0.010785392485558987,\n",
              " 0.010910804383456707,\n",
              " 0.049612805247306824,\n",
              " 0.061200834810733795,\n",
              " 0.008063962683081627,\n",
              " -0.05074150860309601,\n",
              " -0.024718614295125008,\n",
              " -0.04587554186582565,\n",
              " 0.016566865146160126,\n",
              " -0.02944662980735302,\n",
              " -0.0552312433719635,\n",
              " -0.052923671901226044,\n",
              " -0.02149553783237934,\n",
              " -0.04813294857740402,\n",
              " -0.02570936642587185,\n",
              " 0.012748083099722862,\n",
              " -0.00538956094533205,\n",
              " -0.04479700326919556,\n",
              " -0.09807182848453522,\n",
              " -0.041486140340566635,\n",
              " -0.02681298740208149,\n",
              " -0.010666252113878727,\n",
              " 0.05618437007069588,\n",
              " -0.01842295564711094,\n",
              " 0.023364171385765076,\n",
              " 0.003759210929274559,\n",
              " -0.04610128328204155,\n",
              " 0.008910490199923515,\n",
              " 0.013268541544675827,\n",
              " -0.007117105182260275,\n",
              " 0.014133880846202374,\n",
              " -0.04151121899485588,\n",
              " 0.025508707389235497,\n",
              " 0.08267129212617874,\n",
              " -0.016115384176373482,\n",
              " -0.010917074978351593,\n",
              " -0.035667043179273605,\n",
              " 0.04394420608878136,\n",
              " 0.04702932760119438,\n",
              " 0.04023202136158943,\n",
              " 0.025483625009655952,\n",
              " 0.008703561499714851,\n",
              " 0.07780531793832779,\n",
              " -0.024380004033446312,\n",
              " -0.039579883217811584,\n",
              " -0.013481740839779377,\n",
              " 0.045148156583309174,\n",
              " 0.04793229326605797,\n",
              " -0.0067534116096794605,\n",
              " -0.021407751366496086,\n",
              " -0.003796834498643875,\n",
              " 0.0007920523057691753,\n",
              " -0.02774103358387947,\n",
              " 0.025809695944190025,\n",
              " 0.029421547427773476,\n",
              " -0.015563572756946087,\n",
              " -0.0042859395034611225,\n",
              " -0.024530498310923576,\n",
              " 0.012346766889095306,\n",
              " 0.03684591129422188,\n",
              " -0.007944821380078793,\n",
              " -0.025245344266295433,\n",
              " -0.007819409482181072,\n",
              " 0.06426087766885757,\n",
              " 0.03667033463716507,\n",
              " -0.031929779797792435,\n",
              " 0.03815019130706787,\n",
              " 0.034989818930625916,\n",
              " 0.016015054658055305,\n",
              " 0.013933222740888596,\n",
              " -0.0129550127312541,\n",
              " 0.0535256452858448,\n",
              " -0.006897634826600552,\n",
              " -0.006016618572175503,\n",
              " -0.0645618662238121,\n",
              " 0.04108482226729393,\n",
              " -0.03386111557483673,\n",
              " 0.016792606562376022,\n",
              " -0.023414334282279015,\n",
              " 0.014622986316680908,\n",
              " -0.01150650903582573,\n",
              " -0.01720646396279335,\n",
              " 0.01074776891618967,\n",
              " -0.009788370691239834,\n",
              " 0.019150342792272568,\n",
              " -0.017432205379009247,\n",
              " 0.029521876946091652,\n",
              " -0.03024926409125328,\n",
              " -0.000419736752519384,\n",
              " -0.0036870993208140135,\n",
              " 0.06506350636482239,\n",
              " 0.02668757550418377,\n",
              " 0.02473115548491478,\n",
              " -0.05959556624293327,\n",
              " 0.002826462732627988,\n",
              " -0.019626906141638756,\n",
              " 0.03973037749528885,\n",
              " -0.004053927958011627,\n",
              " 0.020768150687217712,\n",
              " 0.023652616888284683,\n",
              " -0.0021508079953491688,\n",
              " -0.0067408704198896885,\n",
              " -0.011487697251141071,\n",
              " -0.0068474700674414635,\n",
              " -0.01404609251767397,\n",
              " -0.03371062129735947,\n",
              " -0.015513407997786999,\n",
              " 0.008076502941548824,\n",
              " -0.058190956711769104,\n",
              " 0.023539746180176735,\n",
              " -0.04758113995194435,\n",
              " -0.021959561854600906,\n",
              " -0.05222136527299881,\n",
              " 0.01377018727362156,\n",
              " -0.007154728751629591,\n",
              " 0.008057692088186741,\n",
              " -0.0017416528426110744,\n",
              " -0.006176518741995096,\n",
              " -0.001798087963834405,\n",
              " 0.0022636784706264734,\n",
              " -0.012327955104410648,\n",
              " 0.016441453248262405,\n",
              " 0.0057030897587537766,\n",
              " 0.033409636467695236,\n",
              " 0.02974761836230755,\n",
              " -0.0031070709228515625,\n",
              " 0.014422327280044556,\n",
              " -0.02528296783566475,\n",
              " 0.03072582744061947,\n",
              " -0.016265876591205597,\n",
              " -0.04610128328204155,\n",
              " -0.007844491861760616,\n",
              " -0.02829284407198429,\n",
              " 0.02969745360314846,\n",
              " -0.03250667080283165,\n",
              " 0.0013763917377218604,\n",
              " -0.00151904730591923,\n",
              " -0.004753097426146269,\n",
              " 0.024204427376389503,\n",
              " 0.06125099956989288,\n",
              " -0.026988564059138298,\n",
              " 0.06360873579978943,\n",
              " -0.0021931342780590057,\n",
              " -0.0407838337123394,\n",
              " -0.020479705184698105,\n",
              " -0.02570936642587185,\n",
              " 0.0503401942551136,\n",
              " -0.01580185443162918,\n",
              " 0.00362752890214324,\n",
              " -0.002986362436786294,\n",
              " -0.0320802740752697,\n",
              " -0.058441780507564545,\n",
              " -0.030224181711673737,\n",
              " 0.025508707389235497,\n",
              " 0.0038814872968941927,\n",
              " -0.007305222563445568,\n",
              " -0.021959561854600906,\n",
              " 0.040081530809402466,\n",
              " -0.07449445873498917,\n",
              " -0.016178088262677193,\n",
              " -0.03739772364497185,\n",
              " -0.021470455452799797,\n",
              " 0.02841825596988201,\n",
              " 0.0288948193192482,\n",
              " -0.008396303281188011,\n",
              " -0.02394106425344944,\n",
              " 0.02761562168598175,\n",
              " 0.024417627602815628,\n",
              " -0.01720646396279335,\n",
              " -0.017106134444475174,\n",
              " 0.017733192071318626,\n",
              " 0.026110682636499405,\n",
              " 0.03318389505147934,\n",
              " 0.03203010931611061,\n",
              " 0.014861267991364002,\n",
              " 0.009688042104244232,\n",
              " -0.03895282372832298,\n",
              " -0.03724722936749458,\n",
              " 0.045047827064991,\n",
              " 0.023953605443239212,\n",
              " 0.04085908085107803,\n",
              " -0.050791673362255096,\n",
              " -0.022448666393756866,\n",
              " -0.0068788230419158936,\n",
              " 0.05794013291597366,\n",
              " -0.007098293397575617,\n",
              " 0.035115230828523636,\n",
              " -0.061551984399557114,\n",
              " 0.05197054520249367,\n",
              " 0.0323060117661953,\n",
              " -0.013569529168307781,\n",
              " 0.03895282372832298,\n",
              " -0.048158030956983566,\n",
              " 0.026662494987249374,\n",
              " -0.010214770212769508,\n",
              " 0.01578931324183941,\n",
              " 0.018197214230895042,\n",
              " -0.009970217943191528,\n",
              " -0.0033610293176025152,\n",
              " -0.011048756539821625,\n",
              " -0.03258191794157028,\n",
              " -0.03461358696222305,\n",
              " 0.00619846535846591,\n",
              " 0.003016147529706359,\n",
              " 0.015137173235416412,\n",
              " 0.002037937520071864,\n",
              " -0.038676921278238297,\n",
              " -0.013331247493624687,\n",
              " 0.01935100182890892,\n",
              " 0.014259292744100094,\n",
              " 0.015262585133314133,\n",
              " 0.00012648929259739816,\n",
              " -0.010835557244718075,\n",
              " -0.018385332077741623,\n",
              " -0.0026195335667580366,\n",
              " -0.03559179604053497,\n",
              " -0.01886189542710781,\n",
              " -0.033760786056518555,\n",
              " 0.031929779797792435,\n",
              " 0.0067283292300999165,\n",
              " 0.03330930694937706,\n",
              " 0.0199655182659626,\n",
              " -0.02142029069364071,\n",
              " -0.032230768352746964,\n",
              " -0.020241422578692436,\n",
              " 0.043994370847940445,\n",
              " -0.0016381882596760988,\n",
              " -6.618398037971929e-05,\n",
              " 0.000671343645080924,\n",
              " -0.03197994455695152,\n",
              " -0.020366834476590157,\n",
              " -0.015287667512893677,\n",
              " 0.0225239135324955,\n",
              " 0.038601674139499664,\n",
              " 0.01264775451272726,\n",
              " 0.04780688136816025,\n",
              " 0.007148458156734705,\n",
              " -0.006154571659862995,\n",
              " 0.017030887305736542,\n",
              " 0.04389404132962227,\n",
              " 0.021470455452799797,\n",
              " -0.012917389161884785,\n",
              " 0.010484404861927032,\n",
              " -0.04070858657360077,\n",
              " 0.011669544503092766,\n",
              " -0.03478916361927986,\n",
              " -0.01868632063269615,\n",
              " -0.015500866807997227,\n",
              " 0.03225585073232651,\n",
              " 0.02382819354534149,\n",
              " 0.023715322837233543,\n",
              " 0.02754037454724312,\n",
              " 0.035240642726421356,\n",
              " 0.026160847395658493,\n",
              " -0.022273089736700058,\n",
              " -0.010816745460033417,\n",
              " 0.01525004394352436,\n",
              " 0.008107855916023254,\n",
              " 0.0022668135352432728,\n",
              " 0.014698232524096966,\n",
              " 0.05136856809258461,\n",
              " 0.037623461335897446,\n",
              " -0.050616100430488586,\n",
              " -0.03157862648367882,\n",
              " 0.028669077903032303,\n",
              " 0.010622357949614525,\n",
              " 0.002519204281270504,\n",
              " 0.0030427975580096245,\n",
              " -0.03396144509315491,\n",
              " 0.03398652747273445,\n",
              " -0.0068662818521261215,\n",
              " -0.053174495697021484,\n",
              " 0.03574229031801224,\n",
              " 0.028518585488200188,\n",
              " -0.05909391865134239,\n",
              " 0.012133566662669182,\n",
              " 0.019639447331428528,\n",
              " 0.05663585290312767,\n",
              " 0.03127763792872429,\n",
              " 0.02313842996954918,\n",
              " -0.08417622745037079,\n",
              " 0.037748873233795166,\n",
              " -0.0012290331069380045,\n",
              " -0.04376862943172455,\n",
              " -0.021884314715862274,\n",
              " -0.022686948999762535,\n",
              " 0.04976329952478409,\n",
              " -0.025985272601246834,\n",
              " -0.026562165468931198,\n",
              " -0.04687883332371712,\n",
              " 0.027339717373251915,\n",
              " -0.0023091400507837534,\n",
              " -0.03797461465001106,\n",
              " 0.013845434412360191,\n",
              " 0.008289703167974949,\n",
              " 0.012773165479302406,\n",
              " -0.017131216824054718,\n",
              " 0.019137801602482796,\n",
              " -0.029722535982728004,\n",
              " 0.08944351226091385,\n",
              " -0.0042828042060136795,\n",
              " -0.0021617815364152193,\n",
              " -0.018460579216480255,\n",
              " -0.027841363102197647,\n",
              " 0.006207871250808239,\n",
              " 0.008866596035659313,\n",
              " 0.0329330712556839,\n",
              " 0.02897006645798683,\n",
              " -0.0005757173639722168,\n",
              " 0.04923657327890396,\n",
              " 0.08006273210048676,\n",
              " 0.003922245930880308,\n",
              " 0.040683504194021225,\n",
              " 0.006088730413466692,\n",
              " -0.019175425171852112,\n",
              " 0.03200502693653107,\n",
              " 0.01276689488440752,\n",
              " -0.0114751560613513,\n",
              " 0.015538490377366543,\n",
              " -0.02786644548177719,\n",
              " -0.012252707965672016,\n",
              " -0.03122747503221035,\n",
              " 0.005151279270648956,\n",
              " 0.004800126887857914,\n",
              " 0.026035435497760773,\n",
              " 0.0028343009762465954,\n",
              " 0.10544602572917938,\n",
              " -0.030826156958937645,\n",
              " -0.04296599328517914,\n",
              " -0.004113498609513044,\n",
              " 0.014196586795151234,\n",
              " -0.0003197994374204427,\n",
              " -0.0033641646150499582,\n",
              " -0.021219633519649506,\n",
              " -0.013155670836567879,\n",
              " 0.01395830512046814,\n",
              " -0.052873507142066956,\n",
              " -0.001204734668135643,\n",
              " 0.05412762239575386,\n",
              " 0.0367455817759037,\n",
              " -0.013506823219358921,\n",
              " -0.026562165468931198,\n",
              " 0.018836813047528267,\n",
              " -0.003213670803233981,\n",
              " 0.030048605054616928,\n",
              " 0.006311336066573858,\n",
              " 0.015475784428417683,\n",
              " -0.008390032686293125,\n",
              " 0.04793229326605797,\n",
              " -0.010490675456821918,\n",
              " 0.0017008940922096372,\n",
              " -0.013506823219358921,\n",
              " -0.013469199649989605,\n",
              " 0.010848098434507847,\n",
              " 0.009656689129769802,\n",
              " 0.0028531125281006098,\n",
              " 0.025132473558187485,\n",
              " -0.016027595847845078,\n",
              " 0.023953605443239212,\n",
              " -0.01886189542710781,\n",
              " 0.01289230678230524,\n",
              " -0.016065219417214394,\n",
              " -0.02119455114006996,\n",
              " -0.02706381119787693,\n",
              " -0.0036682875361293554,\n",
              " 0.0038814872968941927,\n",
              " -0.014685691334307194,\n",
              " -0.032406341284513474,\n",
              " -0.05197054520249367,\n",
              " -0.025684284046292305,\n",
              " -0.02313842996954918,\n",
              " 0.021156927570700645,\n",
              " 3.852975714835338e-05,\n",
              " 0.0004816587024834007,\n",
              " 0.02075560949742794,\n",
              " -0.012114754877984524,\n",
              " -0.06892618536949158,\n",
              " -0.012804518453776836,\n",
              " -0.008139208890497684,\n",
              " -0.03539113700389862,\n",
              " 0.006342688575387001,\n",
              " -0.001256466843187809,\n",
              " 0.015551031567156315,\n",
              " 0.023113347589969635,\n",
              " 0.0019626906141638756,\n",
              " -0.0054365904070436954,\n",
              " -0.010879451408982277,\n",
              " 0.04218844324350357,\n",
              " -0.01627841778099537,\n",
              " -0.026060517877340317,\n",
              " 0.011932908557355404,\n",
              " -0.024292215704917908,\n",
              " -0.016303500160574913,\n",
              " 0.02552124857902527,\n",
              " -0.07314001023769379,\n",
              " 0.019213048741221428,\n",
              " -0.015488325618207455,\n",
              " -0.008596961386501789,\n",
              " 0.02718922309577465,\n",
              " -0.01586456038057804,\n",
              " -0.02234833687543869,\n",
              " 0.009261642582714558,\n",
              " -0.018259920179843903,\n",
              " 0.009374513290822506,\n",
              " 0.0038438637275248766,\n",
              " -0.022436125203967094,\n",
              " 0.037874285131692886,\n",
              " -0.012133566662669182,\n",
              " -0.05046560615301132,\n",
              " 0.03789936751127243,\n",
              " -0.025558872148394585,\n",
              " 0.008277161978185177,\n",
              " 0.019815023988485336,\n",
              " 0.005038408562541008,\n",
              " -0.04329206421971321,\n",
              " -0.025257885456085205,\n",
              " 0.02841825596988201,\n",
              " 0.012622672133147717,\n",
              " -0.02111930400133133,\n",
              " -0.007248787209391594,\n",
              " 0.0007160215172916651,\n",
              " -0.0011592729715630412,\n",
              " -0.04853426665067673,\n",
              " 0.026461835950613022,\n",
              " -0.01169462688267231,\n",
              " -0.012534883804619312,\n",
              " 0.012164919637143612,\n",
              " 0.0024470926728099585,\n",
              " 0.043818794190883636,\n",
              " -0.05618437007069588,\n",
              " 0.026311341673135757,\n",
              " 0.03135288506746292,\n",
              " -0.01144380308687687,\n",
              " 0.0033767058048397303,\n",
              " -0.006007213145494461,\n",
              " -0.03105189837515354,\n",
              " -0.025132473558187485,\n",
              " 0.01928829587996006,\n",
              " -0.009725664742290974,\n",
              " -0.042589761316776276,\n",
              " -0.02491927333176136,\n",
              " 0.01646653562784195,\n",
              " -0.004226368851959705,\n",
              " 0.0036808287259191275,\n",
              " 0.010384075343608856,\n",
              " 0.016015054658055305,\n",
              " 0.012716730125248432,\n",
              " -0.05773947387933731,\n",
              " 0.008634584955871105,\n",
              " 0.016667194664478302,\n",
              " -0.01249098964035511,\n",
              " 0.0077943275682628155,\n",
              " 0.023966146633028984,\n",
              " -0.03050008788704872,\n",
              " -0.014886350370943546,\n",
              " -0.00499137956649065,\n",
              " -0.039404306560754776,\n",
              " -0.058140791952610016,\n",
              " -0.01868632063269615,\n",
              " 0.05046560615301132,\n",
              " 0.029421547427773476,\n",
              " 0.01824737899005413,\n",
              " -0.013256000354886055,\n",
              " 0.027414962649345398,\n",
              " -0.052321694791316986,\n",
              " -0.024091556668281555,\n",
              " -0.00936824269592762,\n",
              " -0.02080577425658703,\n",
              " 0.0043894038535654545,\n",
              " -0.015751689672470093,\n",
              " -0.009932594373822212,\n",
              " -0.007016775663942099,\n",
              " -0.0018341437680646777,\n",
              " 0.05357581004500389,\n",
              " -0.011155356653034687,\n",
              " -0.0035585525911301374,\n",
              " -0.004778179805725813,\n",
              " -0.007060669828206301,\n",
              " -0.03140304982662201,\n",
              " 0.025395836681127548,\n",
              " -0.0019046878442168236,\n",
              " -0.0017353822477161884,\n",
              " -0.0019485818920657039,\n",
              " -0.028618915006518364,\n",
              " 0.011312121525406837,\n",
              " 0.013644776307046413,\n",
              " 0.01512463204562664,\n",
              " -0.0038532696198672056,\n",
              " 0.008515443652868271,\n",
              " 0.016265876591205597,\n",
              " 0.008120397105813026,\n",
              " 0.0023796840105205774,\n",
              " -0.0401567779481411,\n",
              " 0.01451011560857296,\n",
              " -0.034187186509370804,\n",
              " 0.01684276945888996,\n",
              " -0.012302872724831104,\n",
              " -0.0009013954550027847,\n",
              " -0.016002513468265533,\n",
              " 0.023652616888284683,\n",
              " -4.083223393536173e-05,\n",
              " -0.008358679711818695,\n",
              " -0.0016507294494658709,\n",
              " 0.005640384275466204,\n",
              " -0.007041858043521643,\n",
              " -0.019313378259539604,\n",
              " 0.0053237201645970345,\n",
              " -0.01665465347468853,\n",
              " 0.04336731135845184,\n",
              " -0.014798562042415142,\n",
              " -0.011525320820510387,\n",
              " -0.04906099662184715,\n",
              " -0.0067408704198896885,\n",
              " 0.019087636843323708,\n",
              " -0.0068600112572312355,\n",
              " -0.011149086058139801,\n",
              " 0.012942471541464329,\n",
              " 0.0005565137253142893,\n",
              " -0.06105034053325653,\n",
              " -0.002227622549980879,\n",
              " -0.02799185737967491,\n",
              " -0.02791661024093628,\n",
              " 0.04855934903025627,\n",
              " -0.036043278872966766,\n",
              " -0.019125260412693024,\n",
              " -0.01395830512046814,\n",
              " -0.011487697251141071,\n",
              " 0.01029001735150814,\n",
              " -0.048709843307733536,\n",
              " 0.010296287946403027,\n",
              " -0.026185929775238037,\n",
              " -0.029346300289034843,\n",
              " -0.04138581082224846,\n",
              " 0.043492723256349564,\n",
              " 0.003756075631827116,\n",
              " 0.007762974593788385,\n",
              " -0.03421226888895035,\n",
              " -0.009851076640188694,\n",
              " 0.01750745065510273,\n",
              " -0.03882741555571556,\n",
              " -0.04364321753382683,\n",
              " 0.016328582540154457,\n",
              " -0.0020457757636904716,\n",
              " -1.1298061508568935e-06,\n",
              " 0.03807494416832924,\n",
              " 0.02315097115933895,\n",
              " -0.01745728775858879,\n",
              " -0.0019564200192689896,\n",
              " -0.0011240009916946292,\n",
              " -0.04519832134246826,\n",
              " -0.02119455114006996,\n",
              " -0.013256000354886055,\n",
              " 0.021708738058805466,\n",
              " 0.006565294228494167,\n",
              " 0.002942468272522092,\n",
              " -0.009067254140973091,\n",
              " 0.002897006692364812,\n",
              " -0.022059891372919083,\n",
              " -0.0055525959469377995,\n",
              " 0.043016158044338226,\n",
              " -0.016742441803216934,\n",
              " -0.018937142565846443,\n",
              " 0.0189120601862669,\n",
              " 0.005906883627176285,\n",
              " -0.005000785458832979,\n",
              " 0.00564665487036109,\n",
              " -0.012879765592515469,\n",
              " -0.005119926296174526,\n",
              " 0.009198936633765697,\n",
              " -0.041185151785612106,\n",
              " 0.031929779797792435,\n",
              " 0.006452423986047506,\n",
              " -0.010578463785350323,\n",
              " 0.022737111896276474,\n",
              " -0.004079010337591171,\n",
              " 0.0013528770068660378,\n",
              " 0.009123689495027065,\n",
              " -0.03488949313759804,\n",
              " 0.012710459530353546,\n",
              " -0.029647288843989372,\n",
              " -0.02020379900932312,\n",
              " -0.0035083878319710493,\n",
              " 0.00576893100515008,\n",
              " -0.022987935692071915,\n",
              " 0.03973037749528885,\n",
              " 0.005674872547388077,\n",
              " 0.011105191893875599,\n",
              " -0.008258350193500519,\n",
              " 0.0189120601862669,\n",
              " 0.04131056368350983,\n",
              " -0.05387679859995842,\n",
              " -0.01383289322257042,\n",
              " 0.008973196148872375,\n",
              " 0.01867377944290638,\n",
              " 0.04045776277780533,\n",
              " 0.024016311392188072,\n",
              " -0.03629409894347191,\n",
              " 0.045524388551712036,\n",
              " 0.003630664199590683,\n",
              " 0.03807494416832924,\n",
              " 0.008183103054761887,\n",
              " 0.0196519885212183,\n",
              " 0.025884943082928658,\n",
              " -0.007148458156734705,\n",
              " 0.02681298740208149,\n",
              " 0.015387996099889278,\n",
              " 0.029296137392520905,\n",
              " -0.021746361628174782,\n",
              " 0.036519840359687805,\n",
              " 0.014923973940312862,\n",
              " 0.023778028786182404,\n",
              " -0.007135916966944933,\n",
              " 0.04045776277780533,\n",
              " 8.381997758988291e-05,\n",
              " 0.03228093311190605,\n",
              " 0.03887757658958435,\n",
              " -0.007750433403998613,\n",
              " 0.029471712186932564,\n",
              " -0.00838376209139824,\n",
              " -0.030775992199778557,\n",
              " 0.018084345385432243,\n",
              " 0.006903905421495438,\n",
              " -0.0021445374004542828,\n",
              " 0.01843549683690071,\n",
              " -0.008327326737344265,\n",
              " -0.037924449890851974,\n",
              " -0.008803890086710453,\n",
              " -0.02148299664258957,\n",
              " 0.009236560203135014,\n",
              " -0.0012964418856427073,\n",
              " -0.03978054225444794,\n",
              " -0.008095314726233482,\n",
              " -0.00933688972145319,\n",
              " 0.007242516614496708,\n",
              " 0.03471391648054123,\n",
              " -0.01610284298658371,\n",
              " -0.026286259293556213,\n",
              " -0.004847156349569559,\n",
              " -0.06110050529241562,\n",
              " 0.009292995557188988,\n",
              " 0.06666877865791321,\n",
              " 0.015751689672470093,\n",
              " 0.026060517877340317,\n",
              " 0.009324348531663418,\n",
              " 0.01007054653018713,\n",
              " 0.0032575647346675396,\n",
              " 0.024844026193022728,\n",
              " 0.0043894038535654545,\n",
              " -0.0522715300321579,\n",
              " 0.010095628909766674,\n",
              " -0.02093118615448475,\n",
              " -0.021269798278808594,\n",
              " 0.026913316920399666,\n",
              " -0.038300685584545135,\n",
              " -0.024091556668281555,\n",
              " -0.017093593254685402,\n",
              " -0.026537083089351654,\n",
              " -0.020830856636166573,\n",
              " -0.03488949313759804,\n",
              " -0.005251608323305845,\n",
              " -1.1996153261861764e-05,\n",
              " -0.0267628226429224,\n",
              " -0.018661238253116608,\n",
              " -0.027640704065561295,\n",
              " -0.03446309268474579,\n",
              " 0.014748397283256054,\n",
              " -0.009769558906555176,\n",
              " 0.04181220754981041,\n",
              " -0.002973821246996522,\n",
              " -0.040382515639066696,\n",
              " -0.00300674163736403,\n",
              " 0.031377967447042465,\n",
              " 0.018886977806687355,\n",
              " 0.0005827717832289636,\n",
              " 0.019727235659956932,\n",
              " -0.013356328941881657,\n",
              " 0.010848098434507847,\n",
              " -0.008302244357764721,\n",
              " -0.009468571282923222,\n",
              " -0.03892774134874344,\n",
              " -0.013406493701040745,\n",
              " -0.053475480526685715,\n",
              " 0.021658573299646378,\n",
              " -0.0011718141613528132,\n",
              " -0.011669544503092766,\n",
              " -0.0002398495707893744,\n",
              " 0.009844806045293808,\n",
              " -0.006772223394364119,\n",
              " -0.03448817506432533,\n",
              " 0.0014532062923535705,\n",
              " -0.040683504194021225,\n",
              " 0.00845273770391941,\n",
              " 0.021533161401748657,\n",
              " -0.007280140183866024,\n",
              " -0.006427341606467962,\n",
              " 0.007744162809103727,\n",
              " 0.03050008788704872,\n",
              " 0.012102213688194752,\n",
              " -0.013682398945093155,\n",
              " 0.005624707788228989,\n",
              " 0.016065219417214394,\n",
              " 0.024041393771767616,\n",
              " -0.0028891684487462044,\n",
              " 0.0015292370226234198,\n",
              " 0.006709517445415258,\n",
              " 0.016642112284898758,\n",
              " -0.00483775045722723,\n",
              " -0.009249101392924786,\n",
              " 0.010804204270243645,\n",
              " -0.001666405936703086,\n",
              " -0.007581127807497978,\n",
              " 0.011249415576457977,\n",
              " 0.02583477832376957,\n",
              " -0.0010589436860755086,\n",
              " -0.02523280307650566,\n",
              " 0.04906099662184715,\n",
              " -0.018761567771434784,\n",
              " -0.0017102998681366444,\n",
              " 0.0203919168561697,\n",
              " -0.00040170885040424764,\n",
              " 0.015952348709106445,\n",
              " -0.03631918132305145,\n",
              " -0.025684284046292305,\n",
              " -0.0005232013063505292,\n",
              " -0.0026069923769682646,\n",
              " 0.018523285165429115,\n",
              " -0.022210383787751198,\n",
              " 0.01361969392746687,\n",
              " 0.0803135558962822,\n",
              " -0.02064274065196514,\n",
              " -0.012541154399514198,\n",
              " 0.03160370886325836,\n",
              " -0.05884309485554695,\n",
              " -0.0354914665222168,\n",
              " -0.01429691631346941,\n",
              " 0.017833521589636803,\n",
              " 0.044295355677604675,\n",
              " -0.008321056142449379,\n",
              " 0.002688509877771139,\n",
              " 0.012967553921043873,\n",
              " -0.0031744795851409435,\n",
              " -0.03644459322094917,\n",
              " -0.00979464128613472,\n",
              " -0.029923195019364357,\n",
              " 0.007875844836235046,\n",
              " 0.029346300289034843,\n",
              " 0.013720022514462471,\n",
              " 0.0019438789458945394,\n",
              " -0.012327955104410648,\n",
              " 0.004627685993909836,\n",
              " 0.005066626239567995,\n",
              " 0.030324511229991913,\n",
              " -0.018096886575222015,\n",
              " -0.03142813220620155,\n",
              " 0.00622981833294034,\n",
              " -0.006427341606467962,\n",
              " -0.014861267991364002,\n",
              " 0.024467792361974716,\n",
              " -0.027841363102197647,\n",
              " 0.03285782411694527,\n",
              " 0.013795269653201103,\n",
              " 0.018723944202065468,\n",
              " -0.013318706303834915,\n",
              " -0.02247374877333641,\n",
              " -5.961947317700833e-05,\n",
              " -0.003618123009800911,\n",
              " -0.043016158044338226,\n",
              " -0.035968031734228134,\n",
              " 0.026311341673135757,\n",
              " -0.005844178143888712,\n",
              " 0.011757331900298595,\n",
              " -0.01224643737077713,\n",
              " -0.04126039892435074,\n",
              " 0.01389559917151928,\n",
              " -0.001537859090603888,\n",
              " 0.002069290494546294,\n",
              " -0.03200502693653107,\n",
              " 0.013281082734465599,\n",
              " 0.037021487951278687,\n",
              " -0.019187966361641884,\n",
              " 0.014121339656412601,\n",
              " -0.01074776891618967,\n",
              " 0.006339553743600845,\n",
              " -0.018285002559423447,\n",
              " 0.01585201919078827,\n",
              " -0.005928830709308386,\n",
              " -0.020893562585115433,\n",
              " 0.005003920756280422,\n",
              " 0.08001256734132767,\n",
              " 0.006439882796257734,\n",
              " -0.040382515639066696,\n",
              " -0.03285782411694527,\n",
              " -0.028067102655768394,\n",
              " -0.007468257565051317,\n",
              " 0.02099389210343361,\n",
              " -0.022147677838802338,\n",
              " 0.018046721816062927,\n",
              " 0.06335791200399399,\n",
              " -0.021445373073220253,\n",
              " 0.004881644155830145,\n",
              " 0.015964889898896217,\n",
              " -0.027088893577456474,\n",
              " 0.011688356287777424,\n",
              " -0.019752318039536476,\n",
              " -0.013920681551098824,\n",
              " 0.013694940134882927,\n",
              " -0.04702932760119438,\n",
              " 0.0005596490227617323,\n",
              " -0.03228093311190605,\n",
              " 0.010923345573246479,\n",
              " -0.026712657883763313,\n",
              " 0.031553544104099274,\n",
              " 0.045950789004564285,\n",
              " -0.008396303281188011,\n",
              " 0.019990600645542145,\n",
              " -0.0016773793613538146,\n",
              " 0.03559179604053497,\n",
              " 0.013870516791939735,\n",
              " 0.002279354725033045,\n",
              " 0.018661238253116608,\n",
              " -0.011851390823721886,\n",
              " 0.004019440151751041,\n",
              " -0.022373419255018234,\n",
              " 0.028393173590302467,\n",
              " -0.007725351024419069,\n",
              " 0.0358927845954895,\n",
              " -0.021031515672802925,\n",
              " 0.006345823872834444,\n",
              " 0.010484404861927032,\n",
              " 0.03436276316642761,\n",
              " -0.010032923892140388,\n",
              " -0.016792606562376022,\n",
              " -0.01610284298658371,\n",
              " -0.009575171396136284,\n",
              " 0.007380469236522913,\n",
              " 0.004427027422934771,\n",
              " -0.01604013703763485,\n",
              " 0.01868632063269615,\n",
              " -0.036695417016744614,\n",
              " 0.03912840038537979,\n",
              " -0.0259100254625082,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_and_score = db.similarity_search_by_vector(embedding_vector)\n",
        "docs_and_score[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwsENF3jOtZf",
        "outputId": "259a6c61-d7bd-414e-9764-08a22125c660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id='43fe5acf-3269-42ea-9dcc-5e5c9384f322', metadata={'source': '/content/speech.txt'}, page_content='In summary, this paper introduced Word2Vec, a set of highly efficient algorithms (CBOW and Skip-gram) for learning high-quality word embeddings. Its effectiveness stems from simplified model architectures and clever optimization techniques like hierarchical softmax and negative sampling, which enabled training on unprecedentedly large text corpora.')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title save vector db in local - pickle file\n",
        "\n",
        "db.save_local(\"faiss_index\")"
      ],
      "metadata": {
        "id": "wTA3tD9cOxQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "dHY48n7zO--u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = new_db.similarity_search(query)\n",
        "docs[1].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "PPJau5tSPOE4",
        "outputId": "844b9bae-7529-4af2-e610-d8b9bed393f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Performance: The paper found that Skip-gram generally works better for capturing semantic relationships, especially with large datasets, while CBOW can be faster to train.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9uC6ZPhlPdqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}